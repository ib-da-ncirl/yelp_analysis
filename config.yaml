
# TensorFlow preferred device; see https://www.tensorflow.org/guide/gpu
# basic options are; '/cpu:0': the CPU of your machine, or '/GPU:0': Short-hand notation for the first GPU of your machine that is visible to TensorFlow.
# setting 'auto' will pick the available device with the most memory
modelling_device: auto
#modelling_device: /cpu:0

# set maximum memory allocation for gpu in MB
gpu_memory_limit: 1024

# Set if device placements should be logged; true/false
tf_log_device_placement: false

# results folder
results_path_root: ./results

# specify the model to run, the name of an entry in the models list
#run_model: tf_image_eg
#run_model: tf_image_eg_hflip
#run_model: tf_image_eg_hflip_qsvm
run_model: fashion_1
#run_model: inception_v3_eg
#run_model: inception_v3_eg_hflip
#run_model: inception_v3_eg_hflip_cat_v2
#run_model: inception_v3_eg_hflip_ord_v2
#run_model: alexnet
#run_model: resnet50_eg
#run_model: xception_eg
#run_model: vgg16_eg
# or list of models to run
#run_model:
#  - tf_image_eg_hflip
#  - inception_v3_eg_hflip_cat_v2

#model_path: ./results/inception_v3_eg_hflip_cat_v2/200815_2141/best_model
#model_path: ./results/tf_image_eg_hflip/200815_1707/best_model
model_path: ./results/fashion_1/200816_0730/best_model
#model_path: ./results/tf_image_eg_hflip/200810_1740/tf_image_tl
#model_path: ./results/inception_v3_eg_hflip_ord_v2/200810_1309/inception_v3_tl

# -----------------------------------------------------------------------------------------------------------------
# When doing prediction only *** MAKE SURE 'run_model' IS COMPATIBLE WITH THE MODEL FROM 'model_path' ***
# When doing tuning          *** MAKE SURE 'run_model' IS COMPATIBLE WITH THE MODEL FROM 'hyper_model_function' ***
# TODO make configuration easier
# -----------------------------------------------------------------------------------------------------------------

do_training: true
do_prediction: false
do_kerastuning: false

hyper_model_function: fashion1_tuning
max_trials: 3
executions_per_trial: 3
tuning_directory: <results_path_root>/tuning/<hyper_model_function>/{%y%m%d_%H%M}


# preconfigured settings for demos
preconfigured:
  - name: tuning_demo
    do_kerastuning: true

    run_model: fashion_1
    hyper_model_function: fashion1_tuning
    max_trials: 1
    executions_per_trial: 1

    epochs: 3

#    msg: This demo will take many hours to run depending on the resources available.
#    proceed: query

  - name: prediction_demo
    do_prediction: true

    run_model: tf_image_eg_hflip
    model_path: ./results/tf_image_eg_hflip/200815_1707/best_model

  - name: training_demo
    do_training: true

    run_model: tf_image_eg_hflip
    # number of epochs to run a model
    epochs: 3



defaults:
  # default settings, all may be over written for a particular model by specifying the value in the model config
  # NOTE: Do not include any image augmentation settings in the default section!

  # path to the photo dataset csv file
#  dataset_path: ../processed_data/photo_dataset_photo20.csv
  dataset_path: ../processed_data/photo_dataset.csv
  # specify the limit for the number of photos;
  photo_limit: none
  # number of target classes
  # TODO should probably get number of target classes from dataset
  class_count: 9
  # number of epochs to run a model
  epochs: 15

  # path to the photos folder
  photo_path: ../processed_data/photos150_100
  # default image width & height, may be overwritten in model definition by setting 'image_width' & 'image_height'
  # most images are 600px x 400px, so reduce keeping aspect ratio
  image_width: 150
  image_height: 100

  # column in photo dataset csv file that contains the filenames
  x_col: photo_file

  # column in photo dataset csv file that contains the target data
  y_col: stars_cat  # categorical representation; e.g. '1_0' represents 1.0 stars
#  y_col: stars_ord  # ordinal representation; e.g. [1,1,0,0,0,0,0,0,0,0,0] represents 1.0 stars

  # one of "binary", "categorical", "input", "multi_output", "raw", "sparse" or None. Default: "categorical"
  # Mode for yielding the targets:
  # - "binary": 1D numpy array of binary labels
  # - "categorical": 2D numpy array of one-hot encoded labels. Supports multi-label output.
  # - "input": images identical to input images (mainly used to work with autoencoders)
  # - "multi_output": list with the values of the different columns,
  # - "raw": numpy array of values in y_col column(s),
  # - "sparse": 1D numpy array of integer labels
  # - None, no targets are returned
  class_mode: categorical

  # one of "grayscale", "rgb", "rgba". Whether the images will be converted to have 1 or 3 color channels
  color_mode: rgb
  # Size of the batches of data
  batch_size: 32
  # random seed for shuffling and transformations
  seed: 42
  # Fraction of images reserved for validation (sum of training, validation & verification strictly between 0 and 1).
  validation_split: 0.2
  # Fraction of images reserved for verification
  verification_split: 0.05
  # default template for results folder for each model; 'results_path_root/model_name/YYMMDD_HHMM'
  results_path: <results_path_root>/<model_name>/{%y%m%d_%H%M}

  # rescaling factor. Defaults to None. If None or 0, no rescaling is applied, otherwise we multiply the data by the value provided (after applying all other transformations).
  # Note: eval() is used to evaluate if a string expression is supplied; e.g. '1./255.' = 0.00392156862745098
  rescale: 1./255.

  show_val_loss: false
  save_val_loss: true
  save_summary: true
  save_model: true

  confusion_matrix:
    # Normalizes confusion matrix over the true (rows), predicted (columns) conditions or all the population.
    # If None, confusion matrix will not be normalized.
    normalize : none


# Layer parameters
# ----------------
# layer parameters are specified in the form
# <layer_name:
#   <param>: <value>
#   <param>: <value>
# e.g.
#    conv2D_1:
#      filters: 16
#      kernel: 3

# Activation functions
# see https://keras.io/api/layers/activations/ and https://www.tensorflow.org/api_docs/python/tf/keras/activations
# ----------------------------------------------------------------------------------------------------------------
# one of 'elu','exponential','hard_sigmoid','linear','relu','selu','sigmoid','softmax','softplus','softsign','swish' or 'tanh'
#
# Problem type    Output                             Activation function             Loss function
# Regression      Numerical                          Linear (-infinity to infinity)  Mean square error (MSE)
#                 Numerical > 0                      ReLU (0 to infinity)            Mean square error (MSE)
# Classification  Binary                             Sigmoid (0 to 1)                Binary cross entropy (difference between two probability distribution)
#                                                    Tanh (-1 to 1)                  Binary cross entropy (difference between two probability distribution)
# Classification  Single label, multiple classes     Softmax (0 to 1, all sum to 1)  Cross entropy (difference between two probability distribution)
# Classification  Multiple labels, multiple classes  Sigmoid (0 to 1)                Binary cross entropy (difference between two probability distribution)
#
#

# Loss functions
# --------------
# see https://keras.io/api/losses/ and https://www.tensorflow.org/api_docs/python/tf/keras/losses
# some possible values:
# 'binary_crossentropy'             - only two label classes (assumed to be 0 and 1)
# 'categorical_crossentropy'        - labels to be provided in a one_hot representation
# 'sparse_categorical_crossentropy' - labels to be provided as integers
# 'mean_squared_error'

# Optimiser functions
# -------------------
# see https://keras.io/api/optimizers/ and https://www.tensorflow.org/api_docs/python/tf/keras/optimizers
# one of 'adadelta', 'adagrad', 'adam', 'adamax', 'ftrl', 'nadam' , 'rmsprop' or 'sgd'


models:
  - name: tf_image_eg
    desc: TensorFlow Image Classification Tutorial
    # function from models package to call
    function: tf_image_eg
    show_val_loss: false
    save_val_loss: true
    save_summary: true

    conv2D_1:
      filters: 16
      kernel: 3
      padding: same
      activation: relu
    conv2D_2:
      filters: 32
      kernel: 3
      padding: same
      activation: relu
    conv2D_3:
      filters: 64
      kernel: 3
      padding: same
      activation: relu
    dense_1:
      units: 512
      activation: relu

    run1_optimizer: adam

    # training run 1 loss; see 'Loss functions' above
    run1_loss:
      name: categorical_crossentropy
      from_logits: true

    epochs: 25

  - name: tf_image_eg_hflip
    desc: TensorFlow Image Classification Tutorial (Augmentation)
    parent: tf_image_eg

    # Boolean. Randomly flip inputs horizontally.
    horizontal_flip: true
    # Int. Degree range for random rotations.
    rotation_range: 45
    # Float or [lower, upper]. Range for random zoom.
    # If a float, [lower, upper] = [1-zoom_range, 1+zoom_range].
    zoom_range: 0.5
    # Float, 1-D array-like or int
    # - float: fraction of total width, if < 1, or pixels if >= 1.
    # - 1-D array-like: random elements from the array.
    # - int: integer number of pixels from interval (-width_shift_range, +width_shift_range)
    # - With width_shift_range=2 possible values are integers [-1, 0, +1], same as with width_shift_range=[-1, 0, +1], while with width_shift_range=1.0 possible values are floats in the interval [-1.0, +1.0)
    width_shift_range: 0.15
    # Float, 1-D array-like or int
    # - float: fraction of total height, if < 1, or pixels if >= 1.
    # - 1-D array-like: random elements from the array.
    # - int: integer number of pixels from interval (-height_shift_range, +height_shift_range)
    # - With height_shift_range=2 possible values are integers [-1, 0, +1], same as with height_shift_range=[-1, 0, +1], while with height_shift_range=1.0 possible values are floats in the interval [-1.0, +1.0).
    height_shift_range: 0.15
#    # Float. Shear Intensity (Shear angle in counter-clockwise direction in degrees)
#    shear_range: 0.0
#    # Boolean. Set input mean to 0 over the dataset, feature-wise.
#    featurewise_center: false
#    # Boolean. Set each sample mean to 0.
#    samplewise_center: false
#    # Boolean. Divide inputs by std of the dataset, feature-wise.
#    featurewise_std_normalization: false
#    # Boolean. Divide each input by its std.
#    samplewise_std_normalization: false
#    # epsilon for ZCA whitening. Default is 1e-6.
#    zca_epsilon: 1e-6
#    # Boolean. Apply ZCA whitening.
#    zca_whitening: false

  - name: tf_image_eg_hflip_qsvm
    desc: TensorFlow Image Classification Tutorial (Augmentation+QuasiSVM)
    parent: tf_image_eg_hflip

    function: tf_image_eg_qsvm

  - name: fashion_1
    desc: Basic model
    function: fashion_1

    # from hyperparameters identified by tuning

    conv2D_1:
      # hyperparam 'conv_filters'
      filters: 48
      # hyperparam 'conv_kernel'
      kernel: 4
      padding: same
      # hyperparam 'conv_activation'
      activation: softmax

    pooling_1:
      # pool_size: integer or tuple of 2 integers window size over which to take the maximum. `2,2` will take the max value over a 2x2 pooling window.
      #            If only one integer is specified, the same window length will be used for both dimensions.
      # hyperparam 'pool_filters'
      pool_size: 4

    dropout_1:
      # hyperparam 'drop_rate'
      rate: 0.3

    dense_1:
      # hyperparam 'dense_units'
      units: 192
      # hyperparam 'dense_activation'
      activation: softmax

    log_activation: softmax

    # training run 1 optimizer, see 'Optimiser functions' above
    run1_optimizer:
      name: adam
      # hyperparam 'learning_rate'
      learning_rate: 0.0001

    # training run 1 loss; see 'Loss functions' above
    run1_loss:
      name: categorical_crossentropy
#      from_logits: true

    # InceptionV3
    # -----------
  - name: inception_v3_eg
    desc: Fine-tune InceptionV3 on a new set of classes
    # function from models package to call
    function: inception_v3_eg
    # path to the photos folder
    photo_path: ../processed_data/photos299
    # original InceptionV3 images were 299px x 299px, so use resized copies keeping aspect ratio
    image_width: 299
    image_height: 299

  - name: inception_v3_eg_hflip
    desc: Fine-tune InceptionV3 on a new set of classes (Augmentation)
    parent: inception_v3_eg

    # Boolean. Randomly flip inputs horizontally.
    horizontal_flip: true
    # Int. Degree range for random rotations.
    rotation_range: 45
    # Float or [lower, upper]. Range for random zoom.
    # If a float, [lower, upper] = [1-zoom_range, 1+zoom_range].
    zoom_range: 0.5
    # Float, 1-D array-like or int
    # - float: fraction of total width, if < 1, or pixels if >= 1.
    # - 1-D array-like: random elements from the array.
    # - int: integer number of pixels from interval (-width_shift_range, +width_shift_range)
    # - With width_shift_range=2 possible values are integers [-1, 0, +1], same as with width_shift_range=[-1, 0, +1], while with width_shift_range=1.0 possible values are floats in the interval [-1.0, +1.0)
    width_shift_range: 0.15
    # Float, 1-D array-like or int
    # - float: fraction of total height, if < 1, or pixels if >= 1.
    # - 1-D array-like: random elements from the array.
    # - int: integer number of pixels from interval (-height_shift_range, +height_shift_range)
    # - With height_shift_range=2 possible values are integers [-1, 0, +1], same as with height_shift_range=[-1, 0, +1], while with height_shift_range=1.0 possible values are floats in the interval [-1.0, +1.0).
    height_shift_range: 0.15

  - name: inception_v3_eg_hflip_cat_v2
    desc: Fine-tune InceptionV3 on a new set of classes (Augmentation) Ver. 2 [categorical]
    parent: inception_v3_eg_hflip
    # function from models package to call
    function: inception_v3_eg_v2

    epochs: 25

    # column in photo dataset csv file that contains the target data
    y_col: stars_cat  # categorical representation; e.g. '1_0' represents 1.0 stars
    # one of "binary", "categorical", "input", "multi_output", "raw", "sparse" or None
    class_mode: categorical

    dropout_1:
      # Float between 0 and 1. Fraction of the input units to drop.
      rate: 0.5
    dense_1:
      units: 1024
      # activation function, see 'Activation functions' above
      activation: relu

    dropout_2:
      # Float between 0 and 1. Fraction of the input units to drop.
      rate: 0.6
    dense_2:
      units: 512
      # activation function, see 'Activation functions' above
      activation: relu

    # logistic layer activation function, see 'Activation functions' above
    log_activation: softmax

    # training run 1 optimizer, see 'Optimiser functions' above
    run1_optimizer: rmsprop

    # training run 1 loss; see 'Loss functions' above
    run1_loss: categorical_crossentropy

    # training run 2 optimizer, see 'Optimiser functions' above
    run2_optimizer:
      name: sgd
      lr: 0.0001
      momentum: 0.9

    # training run 2 loss; see 'Loss functions' above
    run2_loss: categorical_crossentropy

    # training run 2 number of inception from top to train, e.g. 2 is top 2
    run2_inceptions_to_train: 2

  - name: inception_v3_eg_hflip_ord_v2
    desc: Fine-tune InceptionV3 on a new set of classes (Augmentation) Ver. 2 [ordinal]
    parent: inception_v3_eg_hflip_cat_v2

    # column in photo dataset csv file that contains the target data
    y_col: stars_ord  # ordinal representation; e.g. [1,1,0...] represents 1.0 stars
    # one of "binary", "categorical", "input", "multi_output", "raw", "sparse" or None
    class_mode: raw

    # logistic layer activation function, see 'Activation functions' above
    log_activation: sigmoid

    # training run 1 loss; see 'Loss functions' above
    run1_loss: categorical_crossentropy

    # training run 2 loss; see 'Loss functions' above
    run2_loss: categorical_crossentropy

#    epochs: 1




  - name: inception_v3_eg_hflip_v2_1
    desc: Fine-tune InceptionV3 on a new set of classes (Augmentation) Ver. 2.1
    parent: inception_v3_eg_hflip_v2

    # Boolean. Randomly flip inputs vertically.
    vertical_flip: true
    # Int. Degree range for random rotations.
    rotation_range: 90
    # Float or [lower, upper]. Range for random zoom.
    # If a float, [lower, upper] = [1-zoom_range, 1+zoom_range].
    zoom_range: 0.25
    # Float, 1-D array-like or int
    # - float: fraction of total width, if < 1, or pixels if >= 1.
    # - 1-D array-like: random elements from the array.
    # - int: integer number of pixels from interval (-width_shift_range, +width_shift_range)
    # - With width_shift_range=2 possible values are integers [-1, 0, +1], same as with width_shift_range=[-1, 0, +1], while with width_shift_range=1.0 possible values are floats in the interval [-1.0, +1.0)
    width_shift_range: 0.05
    # Float, 1-D array-like or int
    # - float: fraction of total height, if < 1, or pixels if >= 1.
    # - 1-D array-like: random elements from the array.
    # - int: integer number of pixels from interval (-height_shift_range, +height_shift_range)
    # - With height_shift_range=2 possible values are integers [-1, 0, +1], same as with height_shift_range=[-1, 0, +1], while with height_shift_range=1.0 possible values are floats in the interval [-1.0, +1.0).
    height_shift_range: 0.05


    # ResNet50
    # --------
  - name: resnet50_eg
    desc: Fine-tune ResNet50 on a new set of classes
    # function from models package to call
    function: resnet50_eg
    photo_path: ../processed_data/photos224
    # original ResNet images were 224px x 224px, so use resized copies keeping aspect ratio
    image_width: 224
    image_height: 224

    epochs: 1

    dense_1:
      units: 1024
      # activation function, see 'Activation functions' above
      activation: relu

    # logistic layer activation function, see 'Activation functions' above
    log_activation: softmax

    # training run 1 optimizer, see 'Optimiser functions' above
    run1_optimizer: rmsprop

    # training run 1 loss; see 'Loss functions' above
    run1_loss: categorical_crossentropy

    # training run 2 optimizer, see 'Optimiser functions' above
    run2_optimizer:
      name: sgd
      lr: 0.0001
      momentum: 0.9

    # training run 1 loss; see 'Loss functions' above
    run2_loss: categorical_crossentropy

    # training run 2, train BatchNormalization layers in base model
    run2_train_bn: true

    # AlexNet50
    # --------
  - name: alexnet
    desc: Single GP AlexNet
    # function from models package to call
    function: alexnet
    photo_path: ../processed_data/photos224
    # original ResNet images were 224px x 224px, so use resized copies keeping aspect ratio
    image_width: 224
    image_height: 224

#    epochs: 15

    # global spatial average pooling layer
    gsap_units: 1024
    # activation function, see 'Activation functions' above
    gsap_activation: relu

    # logistic layer activation function, see 'Activation functions' above
    log_activation: softmax

    # training run 1 optimizer, see 'Optimiser functions' above
    run1_optimizer: rmsprop


    # Xception
    # --------
  - name: xception_eg
    desc: Fine-tune Xception on a new set of classes
    # function from models package to call
    function: xception_eg
    photo_path: ../processed_data/photos299
    # original ResNet images were 299px x 299px, so use resized copies keeping aspect ratio
    image_width: 299
    image_height: 299

    epochs: 1

    # global spatial average pooling layer
    gsap_units: 512
    # activation function, see 'Activation functions' above
    gsap_activation: relu

    # logistic layer activation function, see 'Activation functions' above
    log_activation: softmax

    # training run 1 optimizer, see 'Optimiser functions' above
    run1_optimizer: rmsprop

    # training run 1 loss; see 'Loss functions' above
    run1_loss: categorical_crossentropy

    # training run 2 optimizer, see 'Optimiser functions' above
    run2_optimizer:
      name: sgd
      lr: 0.0001
      momentum: 0.9

    # training run 1 loss; see 'Loss functions' above
    run2_loss: categorical_crossentropy

    # VGG16
    # --------
  - name: vgg16_eg
    desc: Fine-tune VGG16 on a new set of classes
    # function from models package to call
    function: vgg16_eg
    photo_path: ../processed_data/photos224
    # original ResNet images were 224px x 224px, so use resized copies keeping aspect ratio
    image_width: 224
    image_height: 224

    epochs: 1

    dense_1:
      units: 1024
      # activation function, see 'Activation functions' above
      activation: relu

    # logistic layer activation function, see 'Activation functions' above
    log_activation: softmax

    # training run 1 optimizer, see 'Optimiser functions' above
    run1_optimizer: rmsprop

    # training run 1 loss; see 'Loss functions' above
    run1_loss: categorical_crossentropy

    # training run 2 optimizer, see 'Optimiser functions' above
    run2_optimizer:
      name: sgd
      lr: 0.0001
      momentum: 0.9

    # training run 1 loss; see 'Loss functions' above
    run2_loss: categorical_crossentropy




